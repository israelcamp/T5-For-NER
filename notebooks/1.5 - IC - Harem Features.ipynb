{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from argparse import Namespace\n",
    "import collections\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.make_harem import get_example_sets\n",
    "from src.input.example import InputExample\n",
    "from src.input.feature import convert_examples_to_features\n",
    "from src.models.modeling_harem import T5ForHarem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = get_example_sets('../data/harem/', merge_O=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    'merge_O': True,\n",
    "    'labels_mode': 'words',\n",
    "    'datapath': '../data/harem',\n",
    "    'remove_accents': False,\n",
    "    'max_length': 170,\n",
    "    'stride': 85,\n",
    "    'target_max_length': 512,\n",
    "    'pretrained_model': 't5-base-pt/'\n",
    "}\n",
    "hparams = Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = T5ForHarem.from_pretrained(hparams.pretrained_model, hparams=hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_iter = iter(model.train_dataloader())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(dl_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = model.get_target_token_ids(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'mãe da senhora? R- Minha mãe sempre em casa, cuidando de nós, cuidando de tudo. P - Eram muitos irmãos? R- Eram sete. P - Sete irmãos, homens e mulheres? R- Bom, não. Sete irmãs e dois irmãos, me lembro bem agora. P - São nove. Bastante gente. E lá na cidade, a senhora estudou na cidade mesmo, lá em [Outro] Conchas [Local]? R- Estudava em [Outro] Fartura [Local]. Não em [Outro] Conchas [Local] foi só mamãe. P - Em [Outro] Fartura [Local]. E qual escola era? R- Só no [Outro] Grupo Escolar de Fartura [Organização]. P - Ficou lá até que época? R- Até [Outro] 50 [Tempo]. P - Casou lá? E Quem era seu marido? Casei lá. [Outro]'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(target_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reconhecer Entidade: mãe da senhora? R- Minha mãe sempre em casa, cuidando de nós, cuidando de tudo. P - Eram muitos irmãos? R- Eram sete. P - Sete irmãos, homens e mulheres? R- Bom, não. Sete irmãs e dois irmãos, me lembro bem agora. P - São nove. Bastante gente. E lá na cidade, a senhora estudou na cidade mesmo, lá em Conchas? R- Estudava em Fartura. Não em Conchas foi só mamãe. P - Em Fartura. E qual escola era? R- Só no Grupo Escolar de Fartura. P - Ficou lá até que época? R- Até 50. P - Casou lá? E Quem era seu marido? Casei lá.'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(batch[0][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "examples = model.get_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "example = examples['valid'][3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class InputSpanFeatures(object):\n",
    "    \"\"\"A single set of features of data.\"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 unique_id,\n",
    "                 doc_span_index,\n",
    "                 tokens,\n",
    "                 token_to_orig_map,\n",
    "                 token_is_max_context,\n",
    "                 input_ids,\n",
    "                 attention_mask,\n",
    "                 label_tags,\n",
    "                 target_ids):\n",
    "        self.unique_id = unique_id\n",
    "        self.doc_span_index = doc_span_index\n",
    "        self.tokens = tokens\n",
    "        self.token_to_orig_map = token_to_orig_map\n",
    "        self.token_is_max_context = token_is_max_context\n",
    "        self.input_ids = input_ids\n",
    "        self.attention_mask = attention_mask\n",
    "        self.label_tags = label_tags\n",
    "        self.target_ids = target_ids\n",
    "        \n",
    "    def start_ignore_index(self, value: int = -100) -> int:\n",
    "        index = len(self.target_ids)\n",
    "        if value in self.target_ids:\n",
    "            index = self.target_ids.index(value)\n",
    "        return index\n",
    "\n",
    "    @property\n",
    "    def clean_target_ids(self,):\n",
    "        return self.target_ids[:self.start_ignore_index()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _check_is_max_context(doc_spans, cur_span_index, position):\n",
    "    \"\"\"Check if this is the 'max context' doc span for the token.\"\"\"\n",
    "\n",
    "    # Because of the sliding window approach taken to scoring documents, a single\n",
    "    # token can appear in multiple documents. E.g.\n",
    "    #  Doc: the man went to the store and bought a gallon of milk\n",
    "    #  Span A: the man went to the\n",
    "    #  Span B: to the store and bought\n",
    "    #  Span C: and bought a gallon of\n",
    "    #  ...\n",
    "    #\n",
    "    # Now the word 'bought' will have two scores from spans B and C. We only\n",
    "    # want to consider the score with \"maximum context\", which we define as\n",
    "    # the *minimum* of its left and right context (the *sum* of left and\n",
    "    # right context will always be the same, of course).\n",
    "    #\n",
    "    # In the example the maximum context for 'bought' would be span C since\n",
    "    # it has 1 left context and 3 right context, while span B has 4 left context\n",
    "    # and 0 right context.\n",
    "    best_score = None\n",
    "    best_span_index = None\n",
    "    for (span_index, doc_span) in enumerate(doc_spans):\n",
    "        end = doc_span.start + doc_span.length - 1\n",
    "        if position < doc_span.start:\n",
    "            continue\n",
    "        if position > end:\n",
    "            continue\n",
    "        num_left_context = position - doc_span.start\n",
    "        num_right_context = end - position\n",
    "        score = min(num_left_context, num_right_context) + \\\n",
    "            0.01 * doc_span.length\n",
    "        if best_score is None or score > best_score:\n",
    "            best_score = score\n",
    "            best_span_index = span_index\n",
    "\n",
    "    return cur_span_index == best_span_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_spanfeatures(example, max_seq_length, tokenizer, doc_stride, max_target_length=512):\n",
    "    \"\"\"Loads a data file into a list of `InputBatch`s.\"\"\"\n",
    "    '''I still need to fix the example'''\n",
    "\n",
    "    prefix = 'Reconhecer Entidade:'\n",
    "    prefix_tokens = tokenizer.tokenize(prefix)\n",
    "    \n",
    "    features = []\n",
    "\n",
    "    tok_to_orig_index = []\n",
    "    orig_to_tok_index = []\n",
    "    all_doc_tokens = []\n",
    "    all_doc_labels = []\n",
    "    all_doc_labels_mask = []\n",
    "    doc_text = example.source_words\n",
    "    doc_labels = example.word_labels\n",
    "    assert len(doc_labels) == len(doc_text)\n",
    "\n",
    "    for (i, token) in enumerate(doc_text):\n",
    "        orig_to_tok_index.append(len(all_doc_tokens))\n",
    "        sub_tokens = tokenizer.tokenize(token)\n",
    "        token_label = doc_labels[i]\n",
    "        for j, sub_token in enumerate(sub_tokens):\n",
    "            tok_to_orig_index.append(i)\n",
    "            all_doc_tokens.append(sub_token)\n",
    "            if j > 0 and token_label != 'O':\n",
    "                _label = f'I-{token_label.split(\"-\")[-1]}'\n",
    "            else:\n",
    "                _label = token_label\n",
    "            all_doc_labels.append(_label)\n",
    "\n",
    "\n",
    "    # The -1 - len(prefix_tokens) accounts for EOS and prefix\n",
    "    max_tokens_for_doc = max_seq_length - 1 - len(prefix_tokens)\n",
    "\n",
    "    # We can have documents that are longer than the maximum sequence length.\n",
    "    # To deal with this we do a sliding window approach, where we take chunks\n",
    "    # of the up to our max length with a stride of `doc_stride`.\n",
    "    _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n",
    "        \"DocSpan\", [\"start\", \"length\"])\n",
    "    doc_spans = []\n",
    "    start_offset = 0\n",
    "    while start_offset < len(all_doc_tokens):\n",
    "        length = len(all_doc_tokens) - start_offset\n",
    "        if length > max_tokens_for_doc:\n",
    "            length = max_tokens_for_doc\n",
    "        doc_spans.append(_DocSpan(start=start_offset, length=length))\n",
    "        if start_offset + length == len(all_doc_tokens):\n",
    "            break\n",
    "        start_offset += min(length, doc_stride)\n",
    "\n",
    "    unique_count = 0\n",
    "    for (doc_span_index, doc_span) in enumerate(doc_spans):\n",
    "        tokens = []\n",
    "        label_tags = []\n",
    "        token_to_orig_map = {}\n",
    "        token_is_max_context = {}\n",
    "\n",
    "        found_start = False\n",
    "        for i in range(doc_span.length):\n",
    "            split_token_index = doc_span.start + i\n",
    "            \n",
    "            if not found_start:\n",
    "                t = all_doc_tokens[split_token_index]\n",
    "                found_start = t.startswith('▁')\n",
    "            \n",
    "            if found_start:\n",
    "                token_to_orig_map[len(\n",
    "                    tokens)] = tok_to_orig_index[split_token_index]\n",
    "\n",
    "                is_max_context = _check_is_max_context(doc_spans, doc_span_index,\n",
    "                                                       split_token_index)\n",
    "                token_is_max_context[len(tokens)] = is_max_context\n",
    "                tokens.append(all_doc_tokens[split_token_index])\n",
    "                label_tags.append(all_doc_labels[split_token_index])\n",
    "\n",
    "        if tokens[-1] == '▁':\n",
    "            tokens = tokens[:-1]\n",
    "            label_tags = label_tags[:-1]\n",
    "            del token_is_max_context[len(tokens)]\n",
    "            \n",
    "        retokenized = tokenizer.tokenize(tokenizer.convert_tokens_to_string(tokens))\n",
    "        assert len(retokenized) == len(tokens), f'{len(retokenized)} - {len(tokens)} / {retokenized} - {tokens}'\n",
    "        assert len(tokens) == len(label_tags) == len(token_is_max_context)\n",
    "        \n",
    "        tokens = prefix_tokens + tokens\n",
    "        tokens.append(tokenizer.eos_token)\n",
    "\n",
    "        input_ids = tokenizer.convert_tokens_to_ids(tokens)\n",
    "\n",
    "        # The mask has 1 for real tokens and 0 for padding tokens. Only real\n",
    "        # tokens are attended to.\n",
    "        attention_mask = [1] * len(input_ids)\n",
    "\n",
    "        # Zero-pad up to the sequence length.\n",
    "        while len(input_ids) < max_seq_length:\n",
    "            input_ids.append(0)\n",
    "            attention_mask.append(0)\n",
    "\n",
    "        assert len(input_ids) == max_seq_length\n",
    "        assert len(attention_mask) == max_seq_length\n",
    "        \n",
    "        \n",
    "        \n",
    "        idx = 0\n",
    "        target_ids = []\n",
    "        while idx < len(label_tags):\n",
    "\n",
    "            label = label_tags[idx]\n",
    "\n",
    "            if label == 'O':\n",
    "                j = idx + 1\n",
    "                while j < len(label_tags) and label_tags[j] == 'O':\n",
    "                    j += 1\n",
    "                # adds the span\n",
    "                _ids = input_ids[len(prefix_tokens) + idx: len(prefix_tokens) + j]\n",
    "                entity = labels2words.get(label, f'<{label}>')\n",
    "                enitity_ids = tokenizer.encode(entity)\n",
    "                \n",
    "                target_ids += _ids + enitity_ids\n",
    "                idx = j\n",
    "            else:\n",
    "                j = idx + 1\n",
    "                ent_label = label.split('-')[-1]\n",
    "                while j < len(label_tags) and label_tags[j] == f'I-{ent_label}':\n",
    "                    j += 1\n",
    "                # adds the span\n",
    "                _ids = input_ids[len(prefix_tokens) + idx: len(prefix_tokens) + j]        \n",
    "                entity = labels2words.get(ent_label, f'<{ent_label}>')\n",
    "                enitity_ids = tokenizer.encode(entity)\n",
    "                \n",
    "                target_ids += _ids + enitity_ids\n",
    "                idx = j\n",
    "        \n",
    "        target_ids += [tokenizer.eos_token_id]\n",
    "        \n",
    "        assert len(target_ids) < max_target_length, f'{len(target_ids)}'\n",
    "        \n",
    "        while len(target_ids) < max_target_length:\n",
    "            target_ids.append(-100) # to ignore on loss\n",
    "\n",
    "        feature = InputSpanFeatures(\n",
    "            unique_id=f'{unique_count}',\n",
    "            doc_span_index=doc_span_index,\n",
    "            tokens=tokens,\n",
    "            token_to_orig_map=token_to_orig_map,\n",
    "            token_is_max_context=token_is_max_context,\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            label_tags=label_tags,\n",
    "            target_ids=target_ids\n",
    "            \n",
    "        )\n",
    "\n",
    "        unique_count += 1\n",
    "\n",
    "        features.append(feature)\n",
    "\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2words = model.labels2words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "entities = list(labels2words.values())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_example_to_spanfeatures(example, 170, model.tokenizer, 85)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = features[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = feat.input_ids\n",
    "target_ids = feat.target_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_tokens = model.tokenizer.convert_ids_to_tokens(input_ids[7:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_string = model.tokenizer.decode(feat.clean_target_ids)\n",
    "for ent in entities:\n",
    "    target_string = target_string.replace(ent, '')\n",
    "target_string = re.sub(' {2,}', ' ', target_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_tokens = model.tokenizer.tokenize(target_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all([it == tt for it, tt in zip(input_tokens, target_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-22-a8d38cebbf35>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtarget_tokens\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget_string\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32massert\u001b[0m \u001b[0mall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_tokens\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_tokens\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for j, example in enumerate(examples['train']):\n",
    "    features = convert_example_to_spanfeatures(example, 170, model.tokenizer, 85)\n",
    "    \n",
    "    for feat in features:\n",
    "        input_ids = feat.input_ids\n",
    "        target_ids = feat.target_ids\n",
    "\n",
    "        input_tokens = model.tokenizer.convert_ids_to_tokens(input_ids[7:])\n",
    "\n",
    "        target_string = model.tokenizer.decode(feat.clean_target_ids)\n",
    "        for ent in entities:\n",
    "            target_string = target_string.replace(ent, '')\n",
    "#         target_string = re.sub(' {2,}', ' ', target_string)\n",
    "\n",
    "        target_tokens = model.tokenizer.tokenize(target_string)\n",
    "\n",
    "        assert all([it == tt for it, tt in zip(input_tokens, target_tokens)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "▁página ▁página\n",
      "▁de ▁de\n",
      "▁O ▁O\n",
      "pin pin\n",
      "ião ião\n",
      "▁e ▁e\n",
      "▁di ▁di\n",
      "ga ga\n",
      "- -\n",
      "nos nos\n",
      "▁o ▁o\n",
      "▁que ▁que\n",
      "▁pensa ▁pensa\n",
      "▁sobre ▁sobre\n",
      "▁este ▁este\n",
      "▁conflito ▁conflito\n",
      ". .\n",
      "▁Formação ▁Formação\n",
      "▁Profissional ▁Profissional\n",
      "▁é ▁é\n",
      "▁a ▁a\n",
      "▁melhor ▁melhor\n",
      "▁opção ▁opção\n",
      "▁Os ▁Os\n",
      "▁alunos ▁alunos\n",
      "▁com ▁com\n",
      "▁o ▁o\n",
      "▁12 ▁12\n",
      "o o\n",
      "▁ano ▁ano\n",
      "▁que ▁que\n",
      "▁não ▁não\n",
      "▁que ▁que\n",
      "rem rem\n",
      "▁seguir ▁seguir\n",
      "▁o ▁o\n",
      "▁Ensino ▁Ensino\n",
      "▁Superior ▁Superior\n",
      "▁devem ▁devem\n",
      "▁aposta ▁aposta\n",
      "r r\n",
      "▁na ▁na\n",
      "▁formação ▁formação\n",
      "▁profissional ▁profissional\n",
      "▁antes ▁antes\n",
      "▁de ▁de\n",
      "▁começar ▁começar\n",
      "em em\n",
      "▁a ▁a\n",
      "▁trabalhar ▁trabalhar\n",
      ". .\n",
      "▁A ▁A\n",
      "▁mensagem ▁mensagem\n",
      "▁foi ▁foi\n",
      "▁deixada ▁deixada\n",
      "▁pela ▁pela\n",
      "▁orientado ▁orientado\n",
      "ra ra\n",
      "▁do ▁do\n",
      "▁ ▁\n",
      "work work\n",
      "s s\n",
      "hop hop\n",
      "▁de ▁de\n",
      "▁Técnica ▁Técnica\n",
      "s s\n",
      "▁de ▁de\n",
      "▁Pro ▁Pro\n",
      "cura cura\n",
      "▁de ▁de\n",
      "▁Em ▁Em\n",
      "pre pre\n",
      "go go\n",
      ", ,\n",
      "▁terça ▁terça\n",
      "- -\n",
      "feira feira\n",
      ", ,\n",
      "▁em ▁em\n",
      "▁Ma ▁Ma\n",
      "fra fra\n",
      "▁ ▁\n",
      ". .\n",
      "▁9 ▁9\n",
      "-4 -4\n",
      "-20 -20\n",
      "03 03\n",
      "▁18 ▁18\n",
      ": :\n",
      "36 36\n",
      "▁0 ▁0\n",
      "▁Com ▁Com\n",
      "en en\n",
      "tários tários\n",
      "▁ ?\n",
      "? ▁Paz\n",
      "▁Paz ▁sim\n",
      "▁sim ,\n",
      ", ▁Guerra\n",
      "▁Guerra ▁não\n",
      "▁não !\n",
      "! ?\n",
      "? ▁Os\n",
      "▁Os ▁alunos\n",
      "▁alunos ▁da\n",
      "▁da ▁Escola\n",
      "▁Escola ▁Secundária\n",
      "▁Secundária ▁José\n",
      "▁José ▁Sara\n",
      "▁Sara ma\n",
      "ma go\n",
      "go ▁juntaram\n",
      "▁juntaram -\n",
      "- se\n",
      "se ▁contra\n",
      "▁contra ▁a\n",
      "▁a ▁Guerra\n",
      "▁Guerra ▁e\n",
      "▁e ▁a\n",
      "▁a ▁violência\n",
      "▁violência .\n",
      ". ▁No\n",
      "▁No ▁21\n",
      "▁21 o\n",
      "o ▁dia\n",
      "▁dia ▁de\n",
      "▁de ▁confrontos\n",
      "▁confrontos ▁no\n",
      "▁no ▁Iraque\n",
      "▁Iraque ▁\n",
      "▁ ,\n",
      ", ▁alunos\n",
      "▁alunos ▁e\n",
      "▁e ▁professores\n",
      "▁professores ▁percorre\n",
      "▁percorre ram\n",
      "ram ▁algumas\n",
      "▁algumas ▁ruas\n",
      "▁ruas ▁da\n",
      "▁da ▁vila\n",
      "▁vila ▁de\n",
      "▁de ▁Ma\n",
      "▁Ma fra\n",
      "fra ▁\n",
      "▁ ,\n",
      ", ▁distribu\n",
      "▁distribu í\n",
      "í ram\n",
      "ram ▁folheto\n",
      "▁folheto s\n",
      "s ▁e\n",
      "▁e ▁solta\n",
      "▁solta ram\n",
      "ram ▁po\n",
      "▁po mba\n",
      "mba s\n",
      "s ▁brancas\n",
      "▁brancas ▁a\n",
      "▁a ▁favor\n",
      "▁favor ▁da\n",
      "▁da ▁Paz\n"
     ]
    }
   ],
   "source": [
    "for it, tt in zip(input_tokens, target_tokens):\n",
    "    print(it, tt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat = features[3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reconhecer Entidade: página de Opinião e diga-nos o que pensa sobre este conflito. Formação Profissional é a melhor opção Os alunos com o 12o ano que não querem seguir o Ensino Superior devem apostar na formação profissional antes de começarem a trabalhar. A mensagem foi deixada pela orientadora do workshop de Técnicas de Procura de Emprego, terça-feira, em Mafra. 9-4-2003 18:36 0 Comentários? Paz sim, Guerra não!? Os alunos da Escola Secundária José Saramago juntaram-se contra a Guerra e a violência. No 21o dia de confrontos no Iraque, alunos e professores percorreram algumas ruas da vila de Mafra, distribuíram folhetos e soltaram pombas brancas a favor da Paz'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(feat.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'página de Opinião e diga-nos o que pensa sobre este conflito. Formação Profissional é a melhor opção Os alunos com o [Outro] 12o [Valor] ano que não querem seguir o Ensino Superior devem apostar na formação profissional antes de começarem a trabalhar. A mensagem foi deixada pela orientadora do workshop de Técnicas de Procura de Emprego, terça-feira, em [Outro] Mafra [Local]. [Outro] 9-4-2003 [Tempo] 18:36 [Tempo] 0 [Valor] Comentários? Paz sim, Guerra não!? Os alunos da [Outro] Escola Secundária José Saramago [Organização] juntaram-se contra a Guerra e a violência. No [Outro] 21o [Valor] dia de confrontos no [Outro] Iraque [Local], alunos e professores percorreram algumas ruas da vila de [Outro] Mafra [Local], distribuíram folhetos e soltaram pombas brancas a favor da Paz [Outro]'"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(feat.clean_target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "512"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat.target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "300"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(feat.input_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[14133, 254, 1035]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat.target_ids[-3:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_examples = []\n",
    "\n",
    "for example in examples['train']:\n",
    "    target = example.target\n",
    "    target_tokens = model.tokenizer.tokenize(target)\n",
    "    n_spans = len(target_tokens) // stride\n",
    "\n",
    "    for i in range(n_spans):\n",
    "        start = stride * i\n",
    "        end = min(stride * i + max_length, len(target_tokens))\n",
    "\n",
    "        span_tokens = target_tokens[start:end]\n",
    "        target_span_words = model.tokenizer.convert_tokens_to_string(\n",
    "            span_tokens).split(' ')\n",
    "        last_ent_index = [i for i, w in enumerate(\n",
    "            target_span_words) if w in possible_endings]\n",
    "        \n",
    "        if len(last_ent_index):\n",
    "            last_ent_index = last_ent_index[-1]\n",
    "            target_span_words = target_span_words[:last_ent_index+1]\n",
    "        else:\n",
    "            target_span_words = ['[Outro]']\n",
    "            \n",
    "        target_span_words = [\n",
    "            w for w in target_span_words if w in example.target_words]\n",
    "        source_span_words = [\n",
    "            w for w in target_span_words if w in example.source_words]\n",
    "\n",
    "        span_examples.append(InputExample(\n",
    "            source_span_words, target_span_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target = example.target\n",
    "target_tokens = model.tokenizer.tokenize(target)\n",
    "\n",
    "n_spans = len(target_tokens) // stride\n",
    "n_spans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "span_examples = []\n",
    "\n",
    "for i in range(n_spans):\n",
    "    start = stride * i\n",
    "    end = min(stride * i + max_length, len(target_tokens))\n",
    "    \n",
    "    span_tokens = target_tokens[start:end]\n",
    "    target_span_words = model.tokenizer.convert_tokens_to_string(span_tokens).split(' ')\n",
    "    last_ent_index = [i for i, w in enumerate(target_span_words) if w in possible_endings][-1]\n",
    "    target_span_words = target_span_words[:last_ent_index+1]\n",
    "    target_span_words = [w for w in target_span_words if w in example.target_words]\n",
    "    source_span_words = [w for w in target_span_words if w in example.source_words]\n",
    "    \n",
    "    span_examples.append(InputExample(source_span_words, target_span_words))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = convert_examples_to_features(span_examples, model.tokenizer, max_length=256, prefix='Reconhecer Entidades:')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Reconhecer Entidades:. O seu pai, Henru James, era um teologo seguidor de Emanuel Swedenborg. Um dos seus irmaos foi o conhecido novelista Henry James. Concluiu os seus estudos de medicina, em 1870, na Universidade de Harvard, onde iniciou a sua carreira como professor de fisiologia em 1872. A partir de 1880 ensinou psicologia e filosofia em Harvard'"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(features[1].source_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'. O seu pai, [Outro] Henru James [Pessoa], era um teologo seguidor de [Outro] Emanuel Swedenborg [Pessoa]. Um dos seus irmaos foi o conhecido novelista [Outro] Henry James [Pessoa]. Concluiu os seus estudos de medicina, em [Outro] 1870 [Tempo], na [Outro] Universidade de Harvard [Organizaç ⁇ o], onde iniciou a sua carreira como professor de fisiologia em [Outro] 1872 [Tempo]. A partir de [Outro] 1880 [Tempo] ensinou psicologia e filosofia em [Outro] Harvard [Organizaç ⁇ o]'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(features[1].target_ids)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# From model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'collections' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-e5c5e8c5ceb3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Documents/Mestrado/t5-ner/src/models/modeling_utils.py\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    178\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_has_cached_datasets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    179\u001b[0m             \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_examples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 180\u001b[0;31m             \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    181\u001b[0m             self.train_dataset, self.val_dataset, self.test_dataset = self.get_datasets(\n\u001b[1;32m    182\u001b[0m                 features)\n",
      "\u001b[0;32m~/Documents/Mestrado/t5-ner/src/models/modeling_harem.py\u001b[0m in \u001b[0;36mget_features\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 129\u001b[0;31m         feature_sets = {\n\u001b[0m\u001b[1;32m    130\u001b[0m             \u001b[0msetname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_examples_to_span_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msetname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    131\u001b[0m         }\n",
      "\u001b[0;32m~/Documents/Mestrado/t5-ner/src/models/modeling_harem.py\u001b[0m in \u001b[0;36m<dictcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    128\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    129\u001b[0m         feature_sets = {\n\u001b[0;32m--> 130\u001b[0;31m             \u001b[0msetname\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert_examples_to_span_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0msetname\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexs\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    131\u001b[0m         }\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Mestrado/t5-ner/src/models/modeling_harem.py\u001b[0m in \u001b[0;36mconvert_examples_to_span_features\u001b[0;34m(self, examples)\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mex\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mexamples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 123\u001b[0;31m             feats = convert_example_to_spanfeatures(\n\u001b[0m\u001b[1;32m    124\u001b[0m                 ex, self.max_length, self.tokenizer, self.stride, self.target_max_length)\n\u001b[1;32m    125\u001b[0m             \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeats\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/Mestrado/t5-ner/src/input/feature.py\u001b[0m in \u001b[0;36mconvert_example_to_spanfeatures\u001b[0;34m(example, max_seq_length, tokenizer, doc_stride, max_target_length)\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;31m# To deal with this we do a sliding window approach, where we take chunks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    210\u001b[0m     \u001b[0;31m# of the up to our max length with a stride of `doc_stride`.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m     _DocSpan = collections.namedtuple(  # pylint: disable=invalid-name\n\u001b[0m\u001b[1;32m    212\u001b[0m         \"DocSpan\", [\"start\", \"length\"])\n\u001b[1;32m    213\u001b[0m     \u001b[0mdoc_spans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'collections' is not defined"
     ]
    }
   ],
   "source": [
    "model.prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "869"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(model.train_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl_test = model.test_dataloader()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(dl_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_ids = model.get_target_token_ids(batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'W. JAMES [Pessoa] Willian James [Pessoa] Willian James [Pessoa], filosofo e psicologo. Foi o mais influente dos pensadores dos [Outro] EUA [Local], criador do pragmatismo. Nasceu e, [Outro] Nova Iorque [Local], a [Outro] 11 de Janeiro de 1842 [Tempo]. O seu pai, [Outro] Henru James [Pessoa], era um teologo seguidor de [Outro] Emanuel Swedenborg [Pessoa]. Um dos seus irmaos foi o conhecido novelista [Outro] Henry James [Pessoa]. Concluiu os seus estudos de medicina, em [Outro] 1870 [Tempo], na [Outro] Universidade de Harvard [Organizaç ⁇ o], onde iniciou a sua carreira como professor de fisiologia em [Outro] 1872 [Tempo]. A partir de [Outro] 1880 [Tempo] ensinou psicologia e filosofia em [Outro] Harvard [Organizaç ⁇ o], universidade que abandonou em [Outro] 1907 [Tempo], proferindo conferencias nas universidades de [Outro] Columbia [Organizaç ⁇ o] e [Outro] Oxford [Organizaç ⁇ o]. Morreu em [Outro] Chocorua [Local], [Outro] New Hampshire [Local], a [Outro] 26 de Agosto de 1910 [Tempo]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.tokenizer.decode(target_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.3 64-bit ('t5ner': conda)",
   "language": "python",
   "name": "python38364bitt5nercondac7c2d3542c9b4d98a6fe6c685e62d5e9"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
