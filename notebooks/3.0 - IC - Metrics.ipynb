{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ZWJ_Yo0DnRLN"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DoywPEf3n2xH"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from argparse import Namespace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import T5Tokenizer, T5ForConditionalGeneration\n",
    "from seqeval.metrics import f1_score, classification_report\n",
    "import pytorch_lightning as pl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from src.data.make_conll2003 import get_example_sets, InputExample\n",
    "from src.models.modeling_t5conll2003 import T5ForConll2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hparams = {\n",
    "    \"experiment_name\": \"Overfit T5 on CoNLL2003\",\n",
    "    \"batch_size\": 2, \"num_workers\": 2,\n",
    "    \"optimizer\": \"Adam\", \"lr\": 5e-3,\n",
    "    \"datapath\": \"../data/conll2003\",\n",
    "    \"shuffle_train\": False,\n",
    "    \"source_max_length\": 128,\n",
    "    \"target_max_length\": 256,\n",
    "    \"labels_mode\": 'tokens',\n",
    "    \"merge_O\": True,\n",
    "#     \"token_weights\": (\n",
    "#        ('<O>', 0.1),\n",
    "#     )\n",
    "}\n",
    "hparams = Namespace(**hparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model = T5ForConll2003.from_pretrained('t5-small', hparams=hparams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Overfit and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "overfit = True\n",
    "overfit_ckpt = 'overfit_tokens_mode.ckpt'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.prepare_data()\n",
    "dl_train = model.train_dataloader()\n",
    "batch = next(iter(dl_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/conda-bld/pytorch_1587428207430/work/torch/csrc/utils/python_arg_parser.cpp:756: UserWarning: This overload of nonzero is deprecated:\n",
      "\tnonzero(Tensor input, *, Tensor out)\n",
      "Consider using one of the following signatures instead:\n",
      "\tnonzero(Tensor input, *, bool as_tuple)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.251261711120605\n",
      "11.20366096496582\n",
      "8.816116333007812\n",
      "6.981232166290283\n",
      "5.593908786773682\n",
      "4.384148120880127\n",
      "3.5685553550720215\n",
      "2.8973278999328613\n",
      "1.8214129209518433\n",
      "1.6491810083389282\n",
      "1.582523226737976\n",
      "1.4420469999313354\n",
      "1.3759634494781494\n",
      "1.4876009225845337\n",
      "1.4521602392196655\n",
      "1.2886043787002563\n",
      "1.2738251686096191\n",
      "1.25042724609375\n",
      "1.2025346755981445\n",
      "1.1466948986053467\n",
      "1.0892550945281982\n",
      "1.026159405708313\n",
      "0.9719151258468628\n",
      "0.9187763333320618\n",
      "0.8655423521995544\n",
      "0.8059073686599731\n",
      "0.7518723607063293\n",
      "0.6992502808570862\n",
      "0.6488634347915649\n",
      "0.5981446504592896\n",
      "0.5494703650474548\n",
      "0.5029447674751282\n",
      "0.4588232934474945\n",
      "0.41660574078559875\n",
      "0.37697726488113403\n",
      "0.3407685160636902\n",
      "0.3074922263622284\n",
      "0.2761213183403015\n",
      "0.24776655435562134\n",
      "0.22275099158287048\n",
      "0.20037338137626648\n",
      "0.1803586333990097\n",
      "0.16294415295124054\n",
      "0.14796634018421173\n",
      "0.13486717641353607\n",
      "0.12353809922933578\n",
      "0.11369551718235016\n",
      "0.10496120899915695\n",
      "0.09715239703655243\n",
      "0.0900764912366867\n"
     ]
    }
   ],
   "source": [
    "if overfit:\n",
    "    device = 'cuda'\n",
    "\n",
    "    model.to(device)\n",
    "    batch = [x.to(device) for x in batch]\n",
    "\n",
    "    optimizer = model.configure_optimizers()\n",
    "\n",
    "    for _ in range(50):\n",
    "        loss = model.training_step(batch, 0)['loss']\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        model.zero_grad()\n",
    "\n",
    "        print(loss.item())\n",
    "\n",
    "    torch.save(model.state_dict(), overfit_ckpt)\n",
    "else:\n",
    "    print(model.load_state_dict(torch.load(overfit_ckpt)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = model.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['<O>', '<PER>', '<ORG>', '<LOC>', '<MISC>']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entities_tokens = model.entities_tokens\n",
    "entities_tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_token_ids = batch[2].cpu()\n",
    "target_token_ids = target_token_ids.where(target_token_ids != -100, torch.tensor(tokenizer.pad_token_id)).cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_token_ids = model.generate(input_ids=batch[0], attention_mask=batch[1], max_length=model.max_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU <ORG> rejects <O> German <MISC> call to boycott <O> British <MISC> lamb. <O> '"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(target_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU <ORG> rejects <O> German <MISC> call to boycott <O> British <MISC> lamb. <O> '"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(predicted_token_ids[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_entities_from_tokens(tokens, tokenizer, entities_tokens, length=0, fill_token='O'):\n",
    "    sequence_entities = [] # will save all the entities\n",
    "    current_entity = [] # will save current entity\n",
    "    if tokens[0] == tokenizer.pad_token:\n",
    "        tokens = tokens[1:]\n",
    "    for token in tokens:\n",
    "        if token in entities_tokens:\n",
    "            entity = token[1:-1] # remove <,>\n",
    "            if entity == 'O':\n",
    "                blabel = ilabel = entity\n",
    "            else:\n",
    "                blabel = f'B-{entity}'\n",
    "                ilabel = f'I-{entity}'\n",
    "            _len = len(current_entity)\n",
    "            sequence_entities += [blabel] + [ilabel] * (_len - 1)\n",
    "            current_entity.clear()\n",
    "        elif token in (tokenizer.eos_token, tokenizer.pad_token):\n",
    "            break\n",
    "        else:\n",
    "            current_entity.append(token)\n",
    "    if length > 0:\n",
    "        seq_len = len(sequence_entities)\n",
    "        if seq_len > length:\n",
    "            sequence_entities = sequence_entities[:length]\n",
    "        elif seq_len < length:\n",
    "            sequence_entities = sequence_entities + [fill_token] * (length - seq_len)\n",
    "    return sequence_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokens(token_ids, tokenizer, entities):\n",
    "    if isinstance(entities, dict):\n",
    "        sentence = tokenizer.decode(token_ids)\n",
    "        for ent, tok in entities.items():\n",
    "            sentence = sentence.replace(ent, tok)\n",
    "        return tokenizer.tokenize(sentence)\n",
    "    else:\n",
    "        return tokenizer.convert_ids_to_tokens(token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_trues_and_preds_entities(target_token_ids, predicted_token_ids,\n",
    "                                tokenizer, entities, fill_token='O'):\n",
    "    assert len(target_token_ids) ==  len(predicted_token_ids) # ensure batch size is the same\n",
    "    all_target_entities = []\n",
    "    all_predicted_entities = []\n",
    "    entities_tokens = list(entities.values()) if isinstance(entities, dict) else entities\n",
    "    for idx in range(len(target_token_ids)):\n",
    "        # convert to tokens\n",
    "        target_tokens = get_tokens(target_token_ids[idx], tokenizer, entities)\n",
    "        predicted_tokens = get_tokens(predicted_token_ids[idx], tokenizer, entities)\n",
    "        # convert to entities\n",
    "        target_entities = get_entities_from_tokens(target_tokens, tokenizer, entities_tokens)\n",
    "        predicted_entities = get_entities_from_tokens(predicted_tokens, tokenizer, entities_tokens, length=len(target_entities), fill_token=fill_token)\n",
    "        # append\n",
    "        all_target_entities.append(target_entities)\n",
    "        all_predicted_entities.append(predicted_entities)\n",
    "    return all_target_entities, all_predicted_entities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_entities, predicted_entities = get_trues_and_preds_entities(target_token_ids, predicted_token_ids, tokenizer, entities=entities2tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([['B-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'],\n",
       "  ['B-PER', 'I-PER', 'I-PER']],\n",
       " [['B-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'],\n",
       "  ['B-PER', 'I-PER', 'I-PER']])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_entities, predicted_entities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Seqeval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "           precision    recall  f1-score   support\n",
      "\n",
      "      PER       1.00      1.00      1.00         1\n",
      "     MISC       1.00      1.00      1.00         2\n",
      "      ORG       1.00      1.00      1.00         1\n",
      "\n",
      "micro avg       1.00      1.00      1.00         4\n",
      "macro avg       1.00      1.00      1.00         4\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(target_entities, predicted_entities))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'], ['B-PER', 'I-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "print(target_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['B-ORG', 'O', 'O', 'B-MISC', 'O', 'O', 'O', 'B-MISC', 'O', 'O', 'O'], ['B-PER', 'I-PER', 'I-PER']]\n"
     ]
    }
   ],
   "source": [
    "print(predicted_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOEQFs1hQw2f4//XEv2338X",
   "collapsed_sections": [],
   "mount_file_id": "1mwxggUBK86BB6y_2WfG7apqZr9bd1I-k",
   "name": "First T5 training.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:t5ner]",
   "language": "python",
   "name": "conda-env-t5ner-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
