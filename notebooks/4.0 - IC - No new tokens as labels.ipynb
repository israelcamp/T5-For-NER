{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gU8M-xzNUjgg"
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "HUELl3rG16w-"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import T5Tokenizer\n",
    "import transformers\n",
    "\n",
    "from src.utils import read_txt\n",
    "from src.input.example import InputExample\n",
    "from src.input.feature import InputFeature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU NNP B-NP B-ORG\\nrejects VBZ B-VP O\\nGerman JJ B-NP B-MISC\\ncall NN I-NP O\\nto TO B-VP O\\nboycott VB I-VP O\\nBritish JJ B-NP B-MISC\\nlamb NN I-NP O\\n. . O O'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filepath = '../data/conll2003/train.txt'\n",
    "text_examples = read_txt(filepath).split('\\n\\n')[1:-1]\n",
    "text_examples[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels2words = {\n",
    "    'O': '[Other]',\n",
    "    'PER': '[Person]',\n",
    "    'LOC': '[Local]',\n",
    "    'MISC': '[Miscellaneous]',\n",
    "    'ORG': '[Organization]'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "3-Ek4DeZYmC4"
   },
   "outputs": [],
   "source": [
    "def convert_text_to_example_with(text, labels2words={}, split_line_by='\\n', split_row_by=' '):\n",
    "    words, labels = [], []\n",
    "    for row in text.split(split_line_by):\n",
    "        ws = row.split(split_row_by)\n",
    "        words.append(ws[0])\n",
    "        labels.append(ws[-1])\n",
    "\n",
    "    source_words = []\n",
    "    target_words = []\n",
    "\n",
    "    i = 0\n",
    "    while len(source_words) < len(words):\n",
    "        w = words[i]\n",
    "        l = labels[i]\n",
    "\n",
    "        if l == 'O':\n",
    "            source_words.append(w)\n",
    "            target_words.extend([w, labels2words.get(l, f'<{l}>')])\n",
    "            i += 1\n",
    "            continue\n",
    "        else: # found a B-ENT\n",
    "            j = i+1\n",
    "            ent_label = labels[i].split('-')[-1]\n",
    "            while j < len(labels) and labels[j] == f'I-{ent_label}':\n",
    "                j += 1\n",
    "            # adds the span\n",
    "            source_words.extend(words[i:j])\n",
    "            target_words.extend(words[i:j] + [labels2words.get(ent_label, f'<{ent_label}>')])\n",
    "            i = j\n",
    "\n",
    "    return InputExample(source_words, target_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yWA0nDaGhwqo"
   },
   "outputs": [],
   "source": [
    "example = convert_text_to_example_with(text_examples[0], labels2words=labels2words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Source: EU rejects German call to boycott British lamb .\n",
      "Target: EU [Organization] rejects [Other] German [Miscellaneous] call [Other] to [Other] boycott [Other] British [Miscellaneous] lamb [Other] . [Other]\n"
     ]
    }
   ],
   "source": [
    "print(example)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = T5Tokenizer.from_pretrained('t5-small')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_example_to_feature(example: InputExample, tokenizer: transformers.PreTrainedTokenizer,\n",
    "                               max_length : int = 512,\n",
    "                               source_max_length: int = None,\n",
    "                               target_max_length: int = None,\n",
    "                               prefix: str = 'Extract Entities:') -> InputFeature:\n",
    "    \n",
    "    \n",
    "    source = f'{prefix} {example.source}'.strip()\n",
    "    target = example.target\n",
    "\n",
    "    source_tokens = tokenizer.tokenize(source)\n",
    "    target_tokens = tokenizer.tokenize(target)\n",
    "\n",
    "    if source_max_length is None:\n",
    "        source_max_length = max_length\n",
    "    if target_max_length is None:\n",
    "        target_max_length = max_length\n",
    "    \n",
    "    _source_max = source_max_length - 1  # we will add eos token to the end of both lists\n",
    "    _target_max = target_max_length - 1\n",
    "    source_tokens = source_tokens[:min(len(source_tokens), _source_max)]\n",
    "    target_tokens = target_tokens[:min(len(target_tokens), _target_max)]\n",
    "\n",
    "    # adding the eos\n",
    "    source_tokens += [tokenizer.eos_token]\n",
    "    target_tokens += [tokenizer.eos_token]\n",
    "\n",
    "    # attention mask\n",
    "    attention_mask = [1] * len(source_tokens)\n",
    "\n",
    "    # padding source\n",
    "    missing_source = max(0, source_max_length - len(source_tokens))\n",
    "    source_tokens += missing_source * [tokenizer.pad_token]\n",
    "    attention_mask += missing_source * [0]\n",
    "    source_token_ids = tokenizer.convert_tokens_to_ids(source_tokens)\n",
    "\n",
    "    # padding target\n",
    "    missing_target = max(0, target_max_length - len(target_tokens))\n",
    "    target_token_ids = tokenizer.convert_tokens_to_ids(\n",
    "        target_tokens) + missing_target * [-100]\n",
    "\n",
    "    assert source_max_length == len(\n",
    "        source_token_ids), f'Max length is {source_max_length} and len(source_token_ids) is {len(source_tokens)}'\n",
    "    assert target_max_length == len(\n",
    "        target_token_ids), f'Max length is {target_max_length} and len(target_token_ids) is {len(target_tokens)}'\n",
    "    assert source_max_length == len(\n",
    "        attention_mask), f'Max length is {source_max_length} and len(attention_mask) is {len(attention_mask)}'\n",
    "\n",
    "    return InputFeature(source_token_ids, target_token_ids, attention_mask, example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature = convert_example_to_feature(example, tokenizer, max_length=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Extract Entities: EU rejects German call to boycott British lamb.'"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(feature.source_token_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'EU [Organization] rejects [Other] German [Miscellaneous] call [Other] to [Other] boycott [Other] British [Miscellaneous] lamb [Other]. [Other]'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.decode(feature.target_ids)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPOuM9eAIOnssbkIpZDS/3O",
   "collapsed_sections": [],
   "mount_file_id": "1rvZCsB34UU_L9pd_CP5kqTr-ZdCJinms",
   "name": "Creating The Dataset.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python [conda env:t5ner]",
   "language": "python",
   "name": "conda-env-t5ner-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
